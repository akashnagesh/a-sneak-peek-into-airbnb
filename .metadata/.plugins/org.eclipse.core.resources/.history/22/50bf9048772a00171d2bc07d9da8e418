package com.BigData.MapReduceAnalysis;

import java.io.File;
import java.io.IOException;
import java.io.PrintStream;
import java.util.HashMap;
import java.util.Map;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hbase.HBaseConfiguration;
import org.apache.hadoop.hbase.HColumnDescriptor;
import org.apache.hadoop.hbase.HTableDescriptor;
import org.apache.hadoop.hbase.TableName;
import org.apache.hadoop.hbase.client.Admin;
import org.apache.hadoop.hbase.client.Connection;
import org.apache.hadoop.hbase.client.ConnectionFactory;
import org.apache.hadoop.hbase.client.Put;
import org.apache.hadoop.hbase.client.Table;
import org.apache.hadoop.hbase.io.ImmutableBytesWritable;
import org.apache.hadoop.hbase.mapreduce.TableMapReduceUtil;
import org.apache.hadoop.hbase.mapreduce.TableReducer;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;

import com.BigData.utils.HBaseTablesName;

/**
 * This is a simple mapreduce to load selected features from the listing.csv to
 * hBase
 * 
 * @author akashnagesh
 *
 */
public class ListingsDataToHbase {
	public static void main(String args[]) throws Exception {
		if (args.length < 1) {
			System.out.println("hadoop jar <jar location> <path to main class> <path to listings.csv> ");
			System.out.println(
					"Note that this job takes only 1 parameter(assuming the data is for Boston) and writes to ListingsData table");
			System.exit(1);
		}
		Configuration conf = HBaseConfiguration.create();
		Job job = Job.getInstance(conf, "put-listingData-to-hbase");
		job.setJarByClass(ListingsDataToHbase.class);
		job.setMapperClass(ListingDataSelectorMapper.class);
		job.setMapOutputKeyClass(Text.class);
		job.setMapOutputValueClass(Text.class);

		FileInputFormat.addInputPath(job, new Path(args[0]));
		TableMapReduceUtil.initTableReducerJob(HBaseTablesName.listingsTable, TableWriterReducer.class, job);
		System.exit(job.waitForCompletion(true) ? 0 : 1);

	}

	protected static class ListingDataSelectorMapper extends Mapper<Object, Text, Text, Text> {
		static {
			File f = new File("/Users/akashnagesh/Desktop/hbasemapper");
			try {
				System.setOut(new PrintStream(f));
			} catch (Exception e) {

			}
		}

		@Override
		protected void map(Object key, Text value, Context context) throws IOException, InterruptedException {
			String[] split = value.toString().split("\t");
			if (split.length != 95)
				return;
			try {
				String seperator = "~@~";
				String valOut = split[1] + seperator + split[4] + seperator + split[15] + seperator + split[17]
						+ seperator + split[48] + seperator + split[49];
				System.out.println(split[0]);
				context.write(new Text((split[0])), new Text(valOut));
			} catch (Exception e) {
				// TODO: handle exception
			}
		}

	}

	protected static class TableWriterReducer extends TableReducer<Text, Text, ImmutableBytesWritable> {
		static {
			File f = new File("/Users/akashnagesh/Desktop/reducer");
			try {
				System.setOut(new PrintStream(f));
			} catch (Exception e) {

			}
		}

		Connection connection;
		Admin admin;
		Table table;
		Map<Integer, String> columnNames;
		static {
			File f = new File("/Users/akashnagesh/Desktop/sysoutfile");
			try {
				System.setOut(new PrintStream(f));
			} catch (Exception e) {

			}
		}

		@Override
		protected void setup(Context context) throws IOException, InterruptedException {
			// TODO Auto-generated method stub
			// place = context.getConfiguration().get("Place");

			columnNames = new HashMap<>();
			columnNames.put(0, "url");
			columnNames.put(1, "name");
			columnNames.put(2, "thumbnail");
			columnNames.put(3, "picture");
			columnNames.put(4, "lat");
			columnNames.put(5, "long");

			connection = ConnectionFactory.createConnection(context.getConfiguration());
			// Get Admin
			admin = connection.getAdmin();

			if (admin.tableExists(TableName.valueOf(HBaseTablesName.listingsTable))) {
				// Use the created table if its already exists
				// table =
				// connection.getTable(TableName.valueOf(HBaseTablesName.tableNameForAnalysisOfListingByPlace));
				table = connection.getTable(TableName.valueOf(HBaseTablesName.listingsTable));

				if (!table.getTableDescriptor().hasFamily(Bytes.toBytes("ListingData"))) {

					// Create the Column family
					HColumnDescriptor hColumnDescriptor = new HColumnDescriptor("ListingData");

					// Add the column family
					admin.addColumn(TableName.valueOf(HBaseTablesName.listingsTable), hColumnDescriptor);

				}
			} else {

				// Create an Table Descriptor with table name
				HTableDescriptor tablefoThisJob = new HTableDescriptor(
						TableName.valueOf(HBaseTablesName.listingsTable));

				// create an column descriptor for the table
				HColumnDescriptor hColumnDescriptor = new HColumnDescriptor("ListingData");

				// add the column family for the table
				tablefoThisJob.addFamily(hColumnDescriptor);

				// This will create an new Table in the HBase
				admin.createTable(tablefoThisJob);
			}
		}

		@Override
		protected void reduce(Text key, Iterable<Text> values, Context context)
				throws IOException, InterruptedException {

			System.out.println("inside reducer " + key.toString());

			Put putToTable = new Put(Bytes.toBytes(key.toString()));

			String[] split = values.iterator().next().toString().split("~@~");

			for (int i = 0; i < split.length; i++) {
				// System.out.println(columnNames.get(i));
				// System.out.println(split[i]);
				putToTable.addColumn(Bytes.toBytes("ListingData"), Bytes.toBytes(columnNames.get(i)),
						Bytes.toBytes(split[i]));

			}

			// Will Write to ListingsAnalyisByPlace HBase Table
			context.write(null, putToTable);

		}

		@Override
		protected void cleanup(Context context) throws IOException, InterruptedException {
			// TODO Auto-generated method stub

			// Close the Connection
			if (connection != null)
				connection.close();

			// Close the table Connection
			if (table != null)
				table.close();
		}
	}

}
